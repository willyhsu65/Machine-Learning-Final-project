{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4720 + 2606 = 7326\n",
    "# import model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "#from sklearn.metrics import f1_score\n",
    "#from matplotlib.animation import FuncAnimation\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename all file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_number(folder_path, new_name):\n",
    "    for count, filename in enumerate(os.listdir(folder_path)):\n",
    "        old_file = os.path.join(folder_path, filename)\n",
    "        # Ensure it's a file\n",
    "        if os.path.isfile(old_file):\n",
    "            # Get file extension\n",
    "            extension = os.path.splitext(filename)[1]\n",
    "            if extension != \".png\":\n",
    "                extension = \".png\"\n",
    "            # Create new file name\n",
    "            new_file = os.path.join(folder_path, f\"{new_name}_{count + 1}{extension}\")\n",
    "            # Rename the file\n",
    "            os.rename(old_file, new_file)\n",
    "    print(\"Files renamed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_and_number(\"H:\\我的雲端硬碟\\jay chou\\\\total_Jay\\\\\", \"RealJay\")\n",
    "rename_and_number(\"H:\\我的雲端硬碟\\jay chou\\\\total_notJay\", \"FakeJay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(image_arr, th=0.4):\n",
    "        image_arr[image_arr > th] = 1.0\n",
    "        image_arr[image_arr <= th] = 0.0\n",
    "        return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitData(data):\n",
    "    split_ratio = 0.1\n",
    "    split_index = int((1 - split_ratio) * len(X_train))\n",
    "    # Split the data into training and validation sets\n",
    "\n",
    "    x_train = X_train[:split_index]\n",
    "    y_train = Y_train[:split_index]\n",
    "    x_val = X_train[split_index:]\n",
    "    y_val = Y_train[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_resize(path):\n",
    "    # path : the image path you want to resize\n",
    "    # img_resized: img have resize to 360*360\n",
    "    #open picture\n",
    "    img = Image.open(path)\n",
    "    # resize to 360*360\n",
    "    img_resized = img.resize((180, 180), Image.LANCZOS)\n",
    "    return img_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_folder(folder_path):\n",
    "    # read images from 'folder_path'\n",
    "    # flat the image and push into 'images' array\n",
    "    print(f'reading from {folder_path}...')\n",
    "    images = []\n",
    "    length = len(os.listdir(folder_path))\n",
    "    i = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        i = i + 1\n",
    "        progress = ((i + 1)/length)*100\n",
    "        if i % 500 == 0:\n",
    "            print(f\"progress: {progress} %\")\n",
    "        if filename.lower().endswith('.png'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # 打開圖片\n",
    "                with Image.open(file_path) as img:\n",
    "                    # 將圖片轉為灰階\n",
    "                    img = img.convert('L')\n",
    "                    # 檢查圖片尺寸是否為 360x360\n",
    "                    if img.size != (180, 180):\n",
    "                        img = image_resize(file_path)\n",
    "                        img.save(file_path)\n",
    "                        print(file_path)\n",
    "                        print(img.size)\n",
    "                    if img.size == (180, 180):\n",
    "                        # 將圖片轉換為 np.array 並進行標準化 (值範圍從 0 到 1)\n",
    "                        img_array = np.array(img) / 255.0\n",
    "                        # 將圖片展平 (flatten)\n",
    "                        img_array = img_array.flatten()\n",
    "                        #print(img_array)\n",
    "                        images.append(img_array)\n",
    "                        #print(images[0])\n",
    "                        #print(f'image size: {len(images[0])}')\n",
    "                    else:\n",
    "                        print(f\"圖片尺寸為 {img.size}, 不是 180x180: {filename}\")\n",
    "            except UnidentifiedImageError:\n",
    "                print(f\"無法識別圖片文件: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"無法讀取圖片: {filename}, 錯誤: {e}\")\n",
    "    print(f\"finish reading from {folder_path}\")\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading from H:\\我的雲端硬碟\\jay chou\\total_notJay\\...\n",
      "progress: 19.217491369390103 %\n",
      "progress: 38.39662447257383 %\n",
      "progress: 57.57575757575758 %\n",
      "progress: 76.75489067894131 %\n",
      "progress: 95.93402378212504 %\n",
      "finish reading from H:\\我的雲端硬碟\\jay chou\\total_notJay\\\n",
      "(2606, 32400)\n",
      "reading from H:\\我的雲端硬碟\\jay chou\\total_Jay\\...\n",
      "progress: 10.612158441008262 %\n",
      "progress: 21.203134929040456 %\n",
      "progress: 31.794111417072656 %\n",
      "progress: 42.385087905104854 %\n",
      "progress: 52.976064393137044 %\n",
      "progress: 63.56704088116925 %\n",
      "progress: 74.15801736920145 %\n",
      "progress: 84.74899385723363 %\n",
      "progress: 95.33997034526584 %\n",
      "finish reading from H:\\我的雲端硬碟\\jay chou\\total_Jay\\\n",
      "[[-1.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " ...\n",
      " [-1.]\n",
      " [-1.]\n",
      " [-1.]]\n",
      "(7326, 32400)\n",
      "(7326, 1)\n"
     ]
    }
   ],
   "source": [
    "folder_path_test = \"G:\\我的雲端硬碟\\ML test\\\\\"\n",
    "preprocess_folder_real = \"H:\\我的雲端硬碟\\jay chou\\\\total_Jay\\\\\"\n",
    "preprocess_folder_fake = \"H:\\我的雲端硬碟\\jay chou\\\\total_notJay\\\\\"\n",
    "preprocess_fake = np.array(read_images_from_folder(preprocess_folder_fake), dtype=\"object\")\n",
    "print(preprocess_fake.shape)\n",
    "preprocess_real = np.array(read_images_from_folder(preprocess_folder_real), dtype=\"object\")\n",
    "\n",
    "real_labels = np.ones((preprocess_real.shape[0], 1))  \n",
    "fake_labels = np.ones((preprocess_fake.shape[0], 1))\n",
    "fake_labels = fake_labels * (-1)\n",
    "print(fake_labels)\n",
    "preprocess_photo = np.concatenate((preprocess_fake, preprocess_real), axis=0)\n",
    "print(preprocess_photo.shape)\n",
    "\n",
    "labels = np.concatenate((fake_labels, real_labels), axis=0)\n",
    "print(labels.shape)\n",
    "for i in range(len(preprocess_photo)):\n",
    "    preprocess_photo[i] = _preprocess(preprocess_photo[i])\n",
    "#preprocess_photo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7326, 32400)\n",
      "(7326, 1)\n",
      "[[0.2235294117647059 0.2235294117647059 0.2235294117647059 ...\n",
      "  0.34901960784313724 0.3686274509803922 0.3686274509803922]\n",
      " [0.0 0.0 0.0 ... 0.3176470588235294 0.32941176470588235\n",
      "  0.32941176470588235]\n",
      " [1.0 1.0 1.0 ... 0.34901960784313724 0.3058823529411765\n",
      "  0.403921568627451]\n",
      " ...\n",
      " [0.25098039215686274 0.2235294117647059 0.23529411764705882 ...\n",
      "  0.396078431372549 0.43137254901960786 0.39215686274509803]\n",
      " [0.03137254901960784 0.03529411764705882 0.03137254901960784 ...\n",
      "  0.25098039215686274 0.25098039215686274 0.25098039215686274]\n",
      " [0.43137254901960786 0.4 0.39215686274509803 ... 0.792156862745098\n",
      "  0.788235294117647 0.8]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "preprocess_photo = np.concatenate((preprocess_fake, preprocess_real), axis=0)\n",
    "print(preprocess_photo.shape)\n",
    "\n",
    "labels = np.concatenate((fake_labels, real_labels), axis=0)\n",
    "print(labels.shape)\n",
    "#for i in range(len(preprocess_photo)):\n",
    "#    preprocess_photo[i] = _preprocess(preprocess_photo[i])\n",
    "#preprocess_photo\n",
    "print(preprocess_photo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_ratio = 0.1\n",
    "split_index = int((1 - split_ratio) * len(preprocess_photo))\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "permutation = list(np.random.permutation(preprocess_photo.shape[0]))\n",
    "shuffled_X = preprocess_photo[permutation, : ]\n",
    "shuffled_Y = labels[permutation, : ]\n",
    "\n",
    "x_train = shuffled_X[:split_index]\n",
    "y_train = shuffled_Y[:split_index]\n",
    "x_val = shuffled_X[split_index:]\n",
    "y_val = shuffled_Y[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6593, 32400)\n",
      "(6593, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "# CNN\n",
    "# SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = np.dot(A, W) + b # matrix multiplication and addition\n",
    "        self.cache = (A, W, b)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = (1 / m) * np.dot(A_prev.T, dZ) #gradient of loss with respect to weights\n",
    "        self.db = (1 / m) * np.sum(dZ, axis=0, keepdims=True) #gradient of loss with respect to bias\n",
    "        dA_prev = np.dot(dZ, W.T)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "    # GRADED FUNCTION: linear_update_parameters\n",
    "    ### START CODE HERE ###\n",
    "      self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "      self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "      ### END CODE HERE ###\n",
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            Z = np.array(Z, dtype=float)\n",
    "\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.maximum(0, Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = np.array(self.cache)\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            dZ = dA * A * (1 - A)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            dZ = dA * (Z > 0)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            s = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            dZ = s - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(len(self.linear)):\n",
    "            A = self.linear[i].forward(A)\n",
    "            A = self.activation[i].forward(A)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = AL - Y\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ=dAL)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = -(Y / (AL + 1e-5) - (1 - Y) / (1 - AL + 1e-5))\n",
    "\n",
    "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dA=dAL)\n",
    "            dA_prev = self.linear[-1].backward(dZ=dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = self.activation[-1].backward(Y=Y)\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ=dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        for i in (range(L-2, -1, -1)):\n",
    "            dZ = self.activation[i].backward(dA=dA_prev)\n",
    "            dA_prev = self.linear[i].backward(dZ=dZ)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(L):\n",
    "            self.linear[i].update(learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(np.multiply(Y, np.log(AL + 1e-5)) + np.multiply(1 - Y, np.log(1 - AL + 1e-5)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss\n",
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(np.multiply(Y, np.log(AL + 1e-5)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss\n",
    "    \n",
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = [(X_train, Y_train)]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            if model.loss_function == 'cross_entropy':\n",
    "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "                    loss = compute_BCE_loss(AL, Y_batch)\n",
    "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "            elif model.loss_function == 'mse': # Regression\n",
    "                loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            model.backward(AL, Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n",
    "def predict(x, y_true, model):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Forward propagation\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "        if y_true is not None:\n",
    "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "    # make y_pred/y_true become one-hot prediction result\n",
    "    if y_true is not None:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_true is not None:\n",
    "        # compute accuracy\n",
    "        correct = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == yp:\n",
    "                correct += 1\n",
    "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "        print(f'f1 score for each class: {f1_scores}')\n",
    "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(layers_dims, activation_fn, loss_function)\n\u001b[1;32m---> 14\u001b[0m model, losses, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrease_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecrease_proportion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[1;32mIn[23], line 330\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, X_train, Y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\u001b[0m\n\u001b[0;32m    327\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[0;32m    333\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate(learning_rate)\n",
      "Cell \u001b[1;32mIn[23], line 221\u001b[0m, in \u001b[0;36mModel.backward\u001b[1;34m(self, AL, Y)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mrange\u001b[39m(L\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    220\u001b[0m     dZ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation[i]\u001b[38;5;241m.\u001b[39mbackward(dA\u001b[38;5;241m=\u001b[39mdA_prev)\n\u001b[1;32m--> 221\u001b[0m     dA_prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dA_prev\n",
      "Cell \u001b[1;32mIn[23], line 42\u001b[0m, in \u001b[0;36mDense.backward\u001b[1;34m(self, dZ)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# GRADED FUNCTION: linear_backward\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m### START CODE HERE ###\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdW \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(A_prev\u001b[38;5;241m.\u001b[39mT, dZ) \u001b[38;5;66;03m#gradient of loss with respect to weights\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m m) \u001b[38;5;241m*\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#gradient of loss with respect to bias\u001b[39;00m\n\u001b[0;32m     43\u001b[0m dA_prev \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dZ, W\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m### END CODE HERE ###\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_function = 'cross_entropy'\n",
    "layers_dims = [x_train.shape[1], 64, 32, y_train.shape[1]]\n",
    "activation_fn = ['relu', 'relu', 'sigmoid']\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10\n",
    "print_loss = True\n",
    "print_freq = 1\n",
    "decrease_freq = 10\n",
    "decrease_proportion = 0.8\n",
    "# You might need to use mini_batch to reduce training time in this part\n",
    "batch_size = 32\n",
    "\n",
    "model = Model(layers_dims, activation_fn, loss_function)\n",
    "model, losses, history = train_model(model, x_train, y_train, learning_rate, num_iterations, batch_size, print_loss, print_freq, decrease_freq, decrease_proportion)\n",
    "print(model.parameters['W'])\n",
    "print(model.parameters['b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(x_train, y_train, model)\n",
    "pred_val = predict(x_val, y_val, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
