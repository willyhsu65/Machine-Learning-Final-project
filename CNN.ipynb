{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool():\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def create_mask_from_window(self, x):\n",
    "        \"\"\"\n",
    "        Creates a mask from an input x to identify the max entry of x.\n",
    "\n",
    "        Arguments:\n",
    "        x -- Array of shape (filter_size, filter_size)\n",
    "\n",
    "        Returns:\n",
    "        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = x == np.max(x)\n",
    "\n",
    "        return mask\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # retrieve dimensions from the input shape\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "\n",
    "        # Step 1: Output Dimension Calculation\n",
    "        n_H = int((n_H_prev - self.pool_size)/self.stride) + 1\n",
    "        n_W = int((n_W_prev - self.pool_size)/self.stride) + 1\n",
    "        n_C = n_C_prev\n",
    "\n",
    "        # initialize output matrix A with zeros\n",
    "        A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Applying Maxpooling\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Store the input in \"cache\" for backward pass\n",
    "        self.cache = A_prev\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from cache\n",
    "        A_prev = self.cache\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        m, n_H, n_W, n_C = dA.shape\n",
    "\n",
    "        # Step 1: Initialize Gradients\n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Pass through the Gradients\n",
    "                        mask = self.create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i][vert_start:vert_end, horiz_start:horiz_end, c] = mask * dA[i, h, w, c]\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Make sure your output shape is correct\n",
    "\n",
    "        assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the flatten layer, a 2-dimensional array of shape (m, (n_H_prev * n_W_prev * n_C_prev))\n",
    "        \"\"\"\n",
    "\n",
    "        # Save information in \"cache\" for the backward pass\n",
    "        self.cache = A_prev.shape\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        A = A_prev.reshape(A_prev.shape[0], -1)\n",
    "        ### END CODE HERE ###\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- Input data, a 2-dimensional array\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- An array with its original shape (the output shape of its' previous layer).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        dA_prev = dA.reshape(self.cache)\n",
    "        ### END CODE HERE ###\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.load to load the data from npz file\n",
    "### START CODE HERE ###\n",
    "data = np.load(\"data.npz\")\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "### END CODE HERE ###\n",
    "\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(y_train[i])\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "# check the shape of training data and testing data\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s' % (X_test.shape, ))\n",
    "\n",
    "#You can split training and validation set here using train_test_split (Optional)\n",
    "### START CODE HERE ###\n",
    "split_ratio = 0.9\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x1_train = X_train[:int(X_train.shape[0] * split_ratio)]\n",
    "y1_train = y_train[:int(y_train.shape[0] * split_ratio)]\n",
    "x_val = X_train[int(X_train.shape[0] * split_ratio):]\n",
    "y_val = y_train[int(y_train.shape[0] * split_ratio):]\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!\n",
    "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # GRADED CODE: Binary classification\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "learning_rate = 3e-5\n",
    "num_iterations = 20\n",
    "batch_size = 64\n",
    "costs = []   # keep track of cost\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Build the model by adding layers\n",
    "model=Model()\n",
    "model.add(Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2))\n",
    "model.add(Activation(\"relu\", None))\n",
    "model.add(MaxPool(pool_size=2, stride=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, 1))\n",
    "model.add(Activation(\"sigmoid\", None))\n",
    "\n",
    "# Loop (gradient descent)\n",
    "for i in range(0, num_iterations):\n",
    "    print(\"epoch: \",i)\n",
    "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "    for batch in mini_batches:\n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        # forward\n",
    "        AL = model.forward(x_batch)\n",
    "\n",
    "        # compute cost\n",
    "        cost = compute_BCE_loss(AL, y_batch)\n",
    "\n",
    "        # backward\n",
    "        dA_prev = model.backward(AL, y_batch)\n",
    "\n",
    "        # update\n",
    "        model.update(learning_rate)\n",
    "\n",
    "    # AL = model.forward(A)\n",
    "    # dA_prev = model.backward(AL=AL, Y=Y)\n",
    "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    costs.append(cost)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### import your HW4 code######\n",
    "from Dense import Dense\n",
    "from Activation import Activation\n",
    "from Loss import compute_BCE_loss\n",
    "from Predict import predict\n",
    "##################################\n",
    "\n",
    "output = {}\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
