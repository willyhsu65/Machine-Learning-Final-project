{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "###### import your HW4 code######\n",
    "from Dense import Dense\n",
    "from Activation import Activation\n",
    "from Loss import compute_BCE_loss\n",
    "from Predict import predict\n",
    "from preprocess_image import preprocess_image\n",
    "##################################\n",
    "\n",
    "output = {}\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "reading from H:\\我的雲端硬碟\\jay chou\\total_notJay\\...\n",
      "progress: 19.217491369390103 %\n",
      "progress: 38.39662447257383 %\n",
      "progress: 57.57575757575758 %\n",
      "progress: 76.75489067894131 %\n",
      "progress: 95.93402378212504 %\n",
      "finish reading from H:\\我的雲端硬碟\\jay chou\\total_notJay\\\n",
      "(2606, 180, 180)\n",
      "reading from H:\\我的雲端硬碟\\jay chou\\total_Jay\\...\n",
      "progress: 10.612158441008262 %\n",
      "progress: 21.203134929040456 %\n",
      "progress: 31.794111417072656 %\n",
      "progress: 42.385087905104854 %\n",
      "progress: 52.976064393137044 %\n",
      "progress: 63.56704088116925 %\n",
      "progress: 74.15801736920145 %\n",
      "progress: 84.74899385723363 %\n",
      "progress: 95.33997034526584 %\n",
      "finish reading from H:\\我的雲端硬碟\\jay chou\\total_Jay\\\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "(7326, 180, 180)\n",
      "(7326, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        ...,\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 0.0],\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 0.0],\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[1.0, 1.0, 1.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 1.0, 1.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 1.0, 1.0, ..., 1.0, 1.0, 1.0],\n",
       "        ...,\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 0.0],\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 0.0],\n",
       "        [1.0, 1.0, 1.0, ..., 0.0, 0.0, 1.0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [0.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 1.0, 0.0]],\n",
       "\n",
       "       [[0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        ...,\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0],\n",
       "        [0.0, 0.0, 0.0, ..., 0.0, 0.0, 0.0]],\n",
       "\n",
       "       [[1.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 0.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 1.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        ...,\n",
       "        [1.0, 1.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 1.0, 0.0, ..., 1.0, 1.0, 1.0],\n",
       "        [1.0, 1.0, 0.0, ..., 1.0, 1.0, 1.0]]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data, labels = preprocess_image()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]\n",
      "\n",
      "\n",
      " [[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]\n",
      "\n",
      "\n",
      " [[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]\n",
      "\n",
      "\n",
      " [[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]\n",
      "\n",
      "\n",
      " [[[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]\n",
      "\n",
      "  [[1.0]\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   ...\n",
      "   [1.0]\n",
      "   [1.0]\n",
      "   [1.0]]]]\n",
      "(7326, 180, 180, 1)\n"
     ]
    }
   ],
   "source": [
    "data = data.reshape(7326,180,180,1)\n",
    "print(data)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data_CNN.npy', data)\n",
    "np.save('labels_CNN.npy', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv():\n",
    "    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):\n",
    "\n",
    "        self.filter_size= filter_size\n",
    "        self.input_channel=input_channel\n",
    "        self.output_channel=output_channel\n",
    "        self.seed = seed\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "\n",
    "        self.parameters = {'W':None, 'b': None}\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "        W -- weight matrix of shape (filter_size, filter_size, input channel size, output channel size)\n",
    "        b -- bias vector of shape (1, 1, 1, output channel size)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        sd = np.sqrt(6.0 / (self.input_channel + self.output_channel))\n",
    "        W = np.random.uniform(-sd, sd, (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        b = np.zeros((1, 1, 1, self.output_channel))\n",
    "\n",
    "        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        assert(b.shape == (1,1,1,self.output_channel))\n",
    "\n",
    "        self.parameters['W'] = W\n",
    "        self.parameters['b'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad all images in the dataset X with zeros. The padding should be applied to both the height and width of each image.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    X_pad = np.pad(X, ((0,0),(pad, pad), (pad, pad), (0,0)), \"constant\", constant_values=(0,0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(self, a_slice_prev, W, b):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        a_slice_prev -- slice of previous activation layer output with shape (filter_size, filter_size, n_C_prev)\n",
    "        W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)\n",
    "        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "        Returns:\n",
    "        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (≈ 3 lines)\n",
    "        # Step 1: Element-wise product to a_slice_prev and W\n",
    "        filtered = a_slice_prev * W\n",
    "        # Step 2: Sum all values to get a single scalar\n",
    "        Z = np.sum(filtered)\n",
    "        # Step 3: Add the bias\n",
    "        Z = Z + np.squeeze(b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return Z\n",
    "\n",
    "Conv.conv_single_step = conv_single_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward(self, A_prev):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    print(A_prev.shape)\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = self.parameters[\"W\"].shape\n",
    "\n",
    "\n",
    "    # Step 1: Output Dimension Calculation\n",
    "    pad = self.pad\n",
    "    stride = self.stride\n",
    "    n_H = int(np.floor(n_H_prev - self.filter_size + 2 * pad)/self.stride + 1)\n",
    "    n_W = int(np.floor(n_W_prev - self.filter_size + 2 * pad)/self.stride + 1)\n",
    "\n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    # Step 2: Padding\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    # Step 3: Loop Through Training Examples\n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filter) of the output volume\n",
    "\n",
    "\n",
    "                    # Step 3-1: Extracting slices\n",
    "                    vert_start = h * self.stride\n",
    "                    vert_end = vert_start + self.filter_size\n",
    "                    horiz_start = w * self.stride\n",
    "                    horiz_end = horiz_start + self.filter_size\n",
    "                    a_slice_prev = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Step 3-2: Applying Filters\n",
    "                    Z[i, h, w, c] = self.conv_single_step(a_slice_prev, self.parameters[\"W\"][:,:,:,c], self.parameters[\"b\"][:,:,:,c])\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # Save information in \"cache\" for the backward pass\n",
    "    self.cache = A_prev\n",
    "\n",
    "    return Z\n",
    "\n",
    "Conv.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dZ):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    A_prev = self.cache\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = self.parameters[\"W\"].shape\n",
    "\n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # Step 1: Initialize Gradients\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Step 2: Padding\n",
    "    A_prev_pad = zero_pad(A_prev, self.pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, self.pad)\n",
    "\n",
    "    # Step 3: Loop Through Training Examples\n",
    "    for i in range(m):                         # loop over the batch of training examples\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "\n",
    "                    # Step 3-1: Extracting slices\n",
    "                    vert_start = h * self.stride\n",
    "                    vert_end = vert_start + self.filter_size\n",
    "                    horiz_start = w * self.stride\n",
    "                    horiz_end = horiz_start + self.filter_size\n",
    "                    a_slice = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Step 3-2: Update the Gradients\n",
    "                    dA_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] += self.parameters[\"W\"][:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] = dW[:,:,:,c] + (1 / m)* a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] = db[:,:,:,c] + (1/m) * dZ[i, h, w, c]\n",
    "\n",
    "        # Step 4: Remove Padding\n",
    "        dA_prev[i, :, :, :] = dA_prev_pad[i, self.pad:-self.pad, self.pad:-self.pad, :]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    self.dW = dW\n",
    "    self.db = db\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "Conv.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    learning rate -- step size\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "    self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "Conv.update = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool():\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def create_mask_from_window(self, x):\n",
    "        \"\"\"\n",
    "        Creates a mask from an input x to identify the max entry of x.\n",
    "\n",
    "        Arguments:\n",
    "        x -- Array of shape (filter_size, filter_size)\n",
    "\n",
    "        Returns:\n",
    "        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = x == np.max(x)\n",
    "\n",
    "        return mask\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # retrieve dimensions from the input shape\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "\n",
    "        # Step 1: Output Dimension Calculation\n",
    "        n_H = int((n_H_prev - self.pool_size)/self.stride) + 1\n",
    "        n_W = int((n_W_prev - self.pool_size)/self.stride) + 1\n",
    "        n_C = n_C_prev\n",
    "\n",
    "        # initialize output matrix A with zeros\n",
    "        A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Applying Maxpooling\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Store the input in \"cache\" for backward pass\n",
    "        self.cache = A_prev\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from cache\n",
    "        A_prev = self.cache\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        m, n_H, n_W, n_C = dA.shape\n",
    "\n",
    "        # Step 1: Initialize Gradients\n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Pass through the Gradients\n",
    "                        mask = self.create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i][vert_start:vert_end, horiz_start:horiz_end, c] = mask * dA[i, h, w, c]\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Make sure your output shape is correct\n",
    "\n",
    "        assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the flatten layer, a 2-dimensional array of shape (m, (n_H_prev * n_W_prev * n_C_prev))\n",
    "        \"\"\"\n",
    "\n",
    "        # Save information in \"cache\" for the backward pass\n",
    "        self.cache = A_prev.shape\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        A = A_prev.reshape(A_prev.shape[0], -1)\n",
    "        ### END CODE HERE ###\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- Input data, a 2-dimensional array\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- An array with its original shape (the output shape of its' previous layer).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        dA_prev = dA.reshape(self.cache)\n",
    "        ### END CODE HERE ###\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        for l in range(len(self.layers)):\n",
    "            A = self.layers[l].forward(A)\n",
    "        ### END CODE HERE ###\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        L = len(self.layers)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        e = 10**(-5)\n",
    "        dAL = (-1)*(Y/(AL + e) - (1 - Y)/(1 - AL + e))\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "        dZ = self.layers[L-1].backward(dA=dAL)\n",
    "        dA_prev = self.layers[L-2].backward(dZ)\n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-2)):\n",
    "            dA_prev = self.layers[l].backward(dA_prev)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        # Only convolution layer and dense layer have to update parameters\n",
    "        ### START CODE HERE ###\n",
    "        for l in range(len(self.layers)):\n",
    "            if self.layers[l].__class__.__name__ == \"Conv\" or self.layers[l].__class__.__name__ == \"Dense\":\n",
    "                self.layers[l].update(learning_rate)\n",
    "        ### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: X=(6593, 180, 180, 1), y=(6593, 1)\n",
      "Val: X=(733, 180, 180, 1)\n"
     ]
    }
   ],
   "source": [
    "# Use np.load to load the data from npz file\n",
    "### START CODE HERE ###\n",
    "split_ratio = 0.1\n",
    "split_index = int((1 - split_ratio) * len(data))\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "permutation = list(np.random.permutation(data.shape[0]))\n",
    "shuffled_X = data[permutation, : ]\n",
    "shuffled_Y = labels[permutation, : ]\n",
    "\n",
    "x1_train = shuffled_X[:split_index]\n",
    "y1_train = shuffled_Y[:split_index]\n",
    "x_val = shuffled_X[split_index:]\n",
    "y_val = shuffled_Y[split_index:]\n",
    "### END CODE HERE ###\n",
    "\n",
    "# plot first few images\n",
    "#for i in range(9):\n",
    "    # define subplot\n",
    "#    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "#    plt.imshow(x_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "#    plt.title(y_train[i])\n",
    "# show the figure\n",
    "#plt.show()\n",
    "\n",
    "# check the shape of training data and testing data\n",
    "print('Train: X=%s, y=%s' % (x1_train.shape, y1_train.shape))\n",
    "print('Val: X=%s' % (x_val.shape, ))\n",
    "\n",
    "#You can split training and validation set here using train_test_split (Optional)\n",
    "### START CODE HERE ###\n",
    "split_ratio = 0.8\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!\n",
    "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # GRADED CODE: Binary classification\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 0: -0.000010\n",
      "epoch:  1\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 1: -0.000010\n",
      "epoch:  2\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 2: -0.000010\n",
      "epoch:  3\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 3: -0.000010\n",
      "epoch:  4\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 4: -0.000010\n",
      "epoch:  5\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 5: -0.000010\n",
      "epoch:  6\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 6: 11.512925\n",
      "epoch:  7\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(64, 180, 180, 1)\n",
      "(1, 180, 180, 1)\n",
      "Cost after iteration 7: -0.000010\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ###\n",
    "learning_rate = 0.003\n",
    "num_iterations = 8\n",
    "batch_size = 64\n",
    "costs = []   # keep track of cost\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Build the model by adding layers\n",
    "model=Model()\n",
    "model.add(Conv(filter_size=4, input_channel=3, output_channel=8, pad=2, stride=3))\n",
    "model.add(MaxPool(pool_size=2, stride=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(7200, 1024))\n",
    "model.add(Activation(\"relu\", None))\n",
    "model.add(Dense(1024, 1))\n",
    "model.add(Activation(\"sigmoid\", None))\n",
    "\n",
    "# Loop (gradient descent)\n",
    "for i in range(0, num_iterations):\n",
    "    print(\"epoch: \",i)\n",
    "    mini_batches = random_mini_batches(x1_train, y1_train, batch_size)\n",
    "\n",
    "    for batch in mini_batches:\n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        # forward\n",
    "        AL = model.forward(x_batch)\n",
    "\n",
    "        # compute cost\n",
    "        cost = compute_BCE_loss(AL, y_batch)\n",
    "\n",
    "        # backward\n",
    "        dA_prev = model.backward(AL, y_batch)\n",
    "\n",
    "        # update\n",
    "        model.update(learning_rate)\n",
    "\n",
    "    # AL = model.forward(A)\n",
    "    # dA_prev = model.backward(AL=AL, Y=Y)\n",
    "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    costs.append(cost)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'W': array([[[[-1.22679122e-01,  3.27077213e-01, -7.39078666e-01,\n",
      "          -2.92005820e-01, -5.21650509e-01, -6.03953543e-01,\n",
      "          -4.62009759e-01, -2.31798884e-01],\n",
      "         [-1.52597046e-01,  5.89724840e-02, -1.20056275e-01,\n",
      "           2.73555653e-01, -4.36427338e-01,  5.56718727e-01,\n",
      "          -6.96680168e-01,  2.48120240e-01],\n",
      "         [-1.22261402e-01,  8.83269899e-02, -5.31882357e-01,\n",
      "          -4.45965334e-01,  4.44354789e-01,  6.89870445e-01,\n",
      "          -2.74176132e-01,  2.80402371e-01]],\n",
      "\n",
      "        [[ 5.55851123e-01,  5.84509039e-01, -6.13628983e-01,\n",
      "          -6.80892888e-01, -4.87567171e-01,  5.56755753e-01,\n",
      "          -5.91866424e-01, -1.20208720e-01],\n",
      "         [ 6.76235160e-01,  5.06247406e-02,  2.82722618e-01,\n",
      "          -2.72533153e-01,  2.75605747e-01,  4.92477133e-01,\n",
      "          -7.10120749e-01,  3.65810681e-01],\n",
      "         [ 7.21983184e-01,  3.68201333e-01, -3.25004379e-01,\n",
      "           4.27262206e-01, -5.85948409e-01, -7.87641039e-02,\n",
      "           6.04950178e-01, -3.08529066e-01]],\n",
      "\n",
      "        [[-3.13589100e-01, -5.44847648e-01, -7.10729140e-01,\n",
      "           2.64125909e-01, -4.25892833e-01, -3.48108276e-01,\n",
      "          -1.11331270e-02, -6.63404202e-01],\n",
      "         [ 1.09366459e-01, -5.20180109e-01,  1.31125934e-01,\n",
      "           2.95030972e-01, -5.87330307e-01, -1.28745460e-01,\n",
      "           2.88462205e-01, -1.30442580e-01],\n",
      "         [-6.64875297e-01,  5.46588736e-02,  2.41153638e-01,\n",
      "           2.19609959e-02,  6.56770646e-01,  1.26052527e-01,\n",
      "           5.97178260e-01, -5.39162310e-01]],\n",
      "\n",
      "        [[-5.32936647e-01,  4.55683393e-01, -1.51928415e-01,\n",
      "          -4.94336290e-01,  6.31532692e-01, -2.26662469e-01,\n",
      "           3.71788170e-01,  3.30144188e-01],\n",
      "         [ 5.66068119e-01,  1.84312324e-01,  3.69879454e-01,\n",
      "          -2.23223621e-01, -3.39778357e-01,  5.82964957e-01,\n",
      "          -1.04902210e-01,  6.82937294e-01],\n",
      "         [ 2.41306592e-01,  1.81392860e-01, -5.69844998e-01,\n",
      "           6.63907956e-01, -7.39240127e-02,  1.13991393e-01,\n",
      "          -1.34376793e-01, -3.92113853e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 5.95718539e-01,  1.10468182e-01, -7.35007854e-01,\n",
      "           1.73002825e-01, -2.55936830e-01,  3.81697246e-02,\n",
      "           5.71488883e-01, -2.14503496e-01],\n",
      "         [ 6.03333910e-01,  1.83851335e-01, -7.15878084e-01,\n",
      "           6.34289153e-01,  2.82099055e-01,  7.32796793e-01,\n",
      "          -4.82570523e-01, -5.39662979e-01],\n",
      "         [ 6.38873346e-01,  2.92356059e-01, -6.41758893e-01,\n",
      "           3.77312256e-01,  3.75125603e-01,  6.23050908e-01,\n",
      "           3.13857396e-01, -5.58665529e-01]],\n",
      "\n",
      "        [[-7.09296542e-01, -6.98196384e-01, -6.97436155e-01,\n",
      "          -3.74902777e-01,  5.31922144e-01,  5.55595421e-02,\n",
      "           7.94378548e-02,  5.01536151e-01],\n",
      "         [-5.55245304e-01, -3.24530954e-01,  1.25976176e-01,\n",
      "           6.93607210e-01,  9.02732286e-02, -7.12802815e-01,\n",
      "           4.45478508e-01, -3.98100097e-01],\n",
      "         [ 4.53511937e-01, -1.64004438e-01,  5.36288244e-01,\n",
      "           3.64991177e-01,  8.31979514e-02, -5.38788961e-01,\n",
      "          -6.48630032e-01, -5.62989743e-01]],\n",
      "\n",
      "        [[-6.72853960e-01, -5.78133226e-01, -4.05941244e-01,\n",
      "           3.14573894e-01,  8.82684977e-02, -7.21800275e-01,\n",
      "          -6.30921747e-01,  6.86535922e-01],\n",
      "         [ 1.00478549e-01, -4.36628569e-01, -3.66626207e-01,\n",
      "           3.60122975e-01, -4.49819802e-01,  1.18377359e-01,\n",
      "           6.95579677e-01,  5.08623132e-01],\n",
      "         [-3.84382827e-01, -7.56637386e-03,  1.76399252e-01,\n",
      "           4.85905313e-01, -5.06892038e-01, -7.12907818e-01,\n",
      "          -6.33805243e-01, -2.38465673e-02]],\n",
      "\n",
      "        [[ 1.56946524e-01,  1.03336681e-01, -2.70560687e-01,\n",
      "           7.21702211e-01,  1.17852165e-01, -1.78840962e-01,\n",
      "           7.65696487e-02,  3.58706011e-01],\n",
      "         [ 2.49861050e-01, -3.45600458e-01, -6.41352988e-01,\n",
      "          -1.91930037e-01,  1.91666126e-01, -4.29899100e-01,\n",
      "           3.74658837e-01, -6.43945009e-01],\n",
      "         [-3.54150563e-01,  4.51788692e-01, -4.53614661e-01,\n",
      "           2.05965693e-01,  3.65011310e-02,  6.25685216e-01,\n",
      "          -3.48319700e-01, -6.44794917e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 3.47102939e-01,  4.03538658e-01,  6.01729971e-01,\n",
      "           6.38033353e-01, -7.18221833e-01, -3.94170944e-01,\n",
      "           1.73971626e-01,  6.59672128e-01],\n",
      "         [ 6.64841697e-01,  8.51873688e-02,  6.13237298e-01,\n",
      "           2.09075469e-01, -1.62750099e-01, -2.24908973e-02,\n",
      "           1.55555355e-01,  6.96181977e-02],\n",
      "         [ 6.29399187e-01,  6.20015339e-01, -1.55932975e-01,\n",
      "           6.84252424e-01, -4.81880123e-01, -5.53745620e-01,\n",
      "          -5.37545245e-01,  4.79464028e-03]],\n",
      "\n",
      "        [[-7.06867201e-01,  6.63200918e-01,  4.82527608e-01,\n",
      "          -7.16396121e-01, -4.78570554e-01, -2.49856281e-01,\n",
      "          -5.43575221e-01,  4.53579116e-01],\n",
      "         [-2.29451663e-01,  6.51586898e-01,  1.20489008e-01,\n",
      "           5.59540245e-01,  5.08925797e-01,  5.97006398e-01,\n",
      "          -5.77822137e-02,  6.48898515e-02],\n",
      "         [ 4.40954235e-01, -3.15009168e-01, -1.50504659e-02,\n",
      "           1.46363946e-01, -7.15885503e-01,  1.36283450e-01,\n",
      "          -9.64879640e-02,  4.50432657e-01]],\n",
      "\n",
      "        [[-2.73014012e-01,  5.81840147e-01,  1.14260340e-01,\n",
      "          -4.66779545e-01,  4.24960802e-01,  1.63680213e-01,\n",
      "          -6.57541593e-01, -1.21450680e-01],\n",
      "         [ 2.64389701e-01,  6.19820868e-01, -7.38697505e-01,\n",
      "           7.04188254e-01, -1.82641819e-01,  6.98023863e-01,\n",
      "           1.56054013e-01,  4.82168517e-01],\n",
      "         [ 1.10243706e-01,  1.90686146e-01, -3.17467211e-01,\n",
      "           1.28229664e-01,  3.68967758e-01,  5.27463810e-01,\n",
      "           3.78159444e-01,  2.88981011e-01]],\n",
      "\n",
      "        [[ 5.38259298e-01, -2.60412462e-01,  2.51529374e-01,\n",
      "          -7.25956851e-02, -1.74484639e-01, -1.33541170e-01,\n",
      "          -1.44146218e-01, -2.73310721e-01],\n",
      "         [ 1.79974341e-01, -1.01526545e-01,  6.99109662e-01,\n",
      "           2.62597642e-01, -4.45580644e-01, -1.10070587e-01,\n",
      "          -2.30014858e-01,  4.36072717e-01],\n",
      "         [ 5.61182171e-01,  5.98019165e-01,  2.39610703e-01,\n",
      "          -3.39456572e-01, -3.66117485e-01,  5.22418200e-01,\n",
      "           4.23153269e-02,  4.42752567e-01]]],\n",
      "\n",
      "\n",
      "       [[[ 1.06960136e-01,  3.45879397e-01,  2.74280701e-02,\n",
      "           4.00090373e-01,  1.01429267e-01, -5.24476073e-02,\n",
      "          -2.30885321e-01, -6.41365994e-01],\n",
      "         [-1.80430437e-01, -6.19428370e-01,  7.12514176e-01,\n",
      "          -4.70320666e-01,  4.60365099e-01,  5.52057314e-01,\n",
      "           2.79783379e-01,  9.90811179e-02],\n",
      "         [-5.00890876e-01, -4.74163846e-02, -2.29350002e-01,\n",
      "          -4.06174579e-01,  1.36368360e-01, -2.79093568e-01,\n",
      "           6.16402616e-01,  6.01502838e-01]],\n",
      "\n",
      "        [[-3.58872556e-01, -5.73246575e-01, -4.54178066e-01,\n",
      "          -6.45900713e-04,  3.37362682e-01, -4.32823121e-01,\n",
      "          -3.70700539e-01,  5.15884852e-01],\n",
      "         [-1.24412181e-01,  1.73860331e-01, -3.94055149e-01,\n",
      "          -5.87965002e-01,  2.31416404e-02, -3.55627411e-02,\n",
      "          -5.11559421e-01,  1.76350795e-01],\n",
      "         [ 6.48947537e-02,  2.29181014e-01, -5.25694998e-01,\n",
      "           3.71499528e-01, -4.10841356e-01,  2.67867977e-02,\n",
      "           4.22888723e-01, -7.09133650e-01]],\n",
      "\n",
      "        [[-2.59546340e-01,  5.52347920e-01,  5.08427446e-01,\n",
      "           5.67488382e-02,  5.41177447e-01,  6.62606679e-01,\n",
      "           4.83513169e-01,  5.19494243e-01],\n",
      "         [-5.92807775e-01,  2.24996374e-01,  2.99872125e-01,\n",
      "           1.62804792e-01,  4.42222209e-01, -6.89284673e-01,\n",
      "           4.00547146e-01,  3.38716895e-01],\n",
      "         [-3.55061497e-01, -3.57327363e-01,  1.94682562e-01,\n",
      "          -2.28542473e-01,  4.37751649e-01, -8.13480899e-02,\n",
      "           4.19026646e-01,  7.20905905e-01]],\n",
      "\n",
      "        [[-2.95165257e-01, -5.25810275e-01,  5.92029456e-01,\n",
      "           6.13555908e-02,  7.00899139e-01,  1.99977269e-01,\n",
      "           7.30935969e-01,  6.44821550e-02],\n",
      "         [ 3.89211912e-02, -5.37003612e-01, -2.13879977e-01,\n",
      "          -6.99853235e-01, -5.01968426e-01,  3.61029377e-01,\n",
      "          -6.92267547e-01, -2.00697842e-01],\n",
      "         [ 5.35108386e-01,  2.86108914e-01,  2.81297847e-01,\n",
      "          -4.59945605e-01, -8.61519266e-02,  1.18697014e-01,\n",
      "           7.24789296e-01, -4.40928423e-01]]]]), 'b': array([[[[-1.12500042e-04,  1.50506394e-03, -7.42388775e-04,\n",
      "          -3.16800997e-05, -3.38862139e-04, -1.80080269e-03,\n",
      "           1.37808125e-03, -3.56893270e-03]]]])}, {'W': array([[-0.00448257,  0.01780779, -0.0241979 , ..., -0.02435598,\n",
      "         0.00179001,  0.02105814],\n",
      "       [ 0.01190219,  0.00940653, -0.02413481, ...,  0.00927366,\n",
      "        -0.0006107 , -0.00170916],\n",
      "       [-0.02700441, -0.01246094,  0.00037937, ...,  0.01376312,\n",
      "         0.01743952,  0.00996932],\n",
      "       ...,\n",
      "       [-0.00670326, -0.002286  , -0.00120137, ..., -0.02116717,\n",
      "        -0.02557661,  0.01607074],\n",
      "       [-0.00166869,  0.008611  ,  0.02433895, ..., -0.01726336,\n",
      "        -0.00442195,  0.00385043],\n",
      "       [ 0.02416596, -0.02424144, -0.00877853, ..., -0.02059279,\n",
      "         0.02633641,  0.01386745]]), 'b': array([[ 0.00000000e+00,  4.02705314e-05,  0.00000000e+00, ...,\n",
      "         2.14693214e-05, -1.23639279e-05,  0.00000000e+00]])}, {'W': array([[-0.01269716],\n",
      "       [ 0.03442989],\n",
      "       [-0.0764917 ],\n",
      "       ...,\n",
      "       [ 0.01800906],\n",
      "       [-0.00884833],\n",
      "       [ 0.05320457]]), 'b': array([[0.00119449]])}]\n"
     ]
    }
   ],
   "source": [
    "weight = []\n",
    "\n",
    "for i in range(len(model.layers)):\n",
    "    if model.layers[i].__class__.__name__ == \"Conv\" or model.layers[i].__class__.__name__ == \"Dense\":\n",
    "        weight.append(model.layers[i].parameters)\n",
    "print(weight)\n",
    "\n",
    "np.save('CNN_model.npy', weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733, 180, 180, 1)\n",
      "Accuracy: 0.660300136425648\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_val = predict(model, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9.999950000398841e-06, -9.999950000398841e-06, -9.999950000398841e-06, -9.999950000398841e-06, -9.999950000398841e-06, -9.999950000398841e-06, 11.512925464970229, -9.999950000398841e-06]\n"
     ]
    }
   ],
   "source": [
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
