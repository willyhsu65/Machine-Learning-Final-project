{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "###### import your HW4 code######\n",
    "from Dense import Dense\n",
    "from Activation import Activation\n",
    "from Loss import compute_BCE_loss\n",
    "from Predict import predict\n",
    "##################################\n",
    "\n",
    "output = {}\n",
    "seed = 1\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv():\n",
    "    def __init__(self, filter_size=2, input_channel=3, output_channel=8, pad=1, stride=1, seed=1):\n",
    "\n",
    "        self.filter_size= filter_size\n",
    "        self.input_channel=input_channel\n",
    "        self.output_channel=output_channel\n",
    "        self.seed = seed\n",
    "        self.pad = pad\n",
    "        self.stride = stride\n",
    "\n",
    "        self.parameters = {'W':None, 'b': None}\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        self.parameters -- python dictionary containing your parameters:\n",
    "                           W -- weight matrix of shape (filter_size, filter_size, input channel size, output channel size)\n",
    "                           b -- bias vector of shape (1, 1, 1, output channel size)\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        sd = np.sqrt(6.0 / (self.input_channel + self.output_channel))\n",
    "        W = np.random.uniform(-sd, sd, (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        b = np.zeros((1, 1, 1, self.output_channel))\n",
    "\n",
    "        assert(W.shape == (self.filter_size,self.filter_size,self.input_channel,self.output_channel))\n",
    "        assert(b.shape == (1,1,1,self.output_channel))\n",
    "\n",
    "        self.parameters['W'] = W\n",
    "        self.parameters['b'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad all images in the dataset X with zeros. The padding should be applied to both the height and width of each image.\n",
    "\n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C), where m represent the number of examples.\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "\n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    X_pad = np.pad(X, ((0,0),(pad, pad), (pad, pad), (0,0)), \"constant\", constant_values=(0,0))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(self, a_slice_prev, W, b):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        a_slice_prev -- slice of previous activation layer output with shape (filter_size, filter_size, n_C_prev)\n",
    "        W -- Weight parameters contained in a window - matrix of shape (filter_size, filter_size, n_C_prev)\n",
    "        b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "\n",
    "        Returns:\n",
    "        Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ### (â‰ˆ 3 lines)\n",
    "        # Step 1: Element-wise product to a_slice_prev and W\n",
    "        filtered = a_slice_prev * W\n",
    "        # Step 2: Sum all values to get a single scalar\n",
    "        Z = np.sum(filtered)\n",
    "        # Step 3: Add the bias\n",
    "        Z = Z + np.squeeze(b)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return Z\n",
    "\n",
    "Conv.conv_single_step = conv_single_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, A_prev):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = self.parameters[\"W\"].shape\n",
    "\n",
    "\n",
    "    # Step 1: Output Dimension Calculation\n",
    "    pad = self.pad\n",
    "    stride = self.stride\n",
    "    n_H = int(np.floor(n_H_prev - self.filter_size + 2 * pad)/self.stride + 1)\n",
    "    n_W = int(np.floor(n_W_prev - self.filter_size + 2 * pad)/self.stride + 1)\n",
    "\n",
    "    # Initialize the output volume Z with zeros\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    # Step 2: Padding\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    # Step 3: Loop Through Training Examples\n",
    "    for i in range(m):                                 # loop over the batch of training examples\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filter) of the output volume\n",
    "\n",
    "\n",
    "                    # Step 3-1: Extracting slices\n",
    "                    vert_start = h * self.stride\n",
    "                    vert_end = vert_start + self.filter_size\n",
    "                    horiz_start = w * self.stride\n",
    "                    horiz_end = horiz_start + self.filter_size\n",
    "                    a_slice_prev = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Step 3-2: Applying Filters\n",
    "                    Z[i, h, w, c] = self.conv_single_step(a_slice_prev, self.parameters[\"W\"][:,:,:,c], self.parameters[\"b\"][:,:,:,c])\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "    # Save information in \"cache\" for the backward pass\n",
    "    self.cache = A_prev\n",
    "\n",
    "    return Z\n",
    "\n",
    "Conv.forward = forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, dZ):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution layer\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "                numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    A_prev = self.cache\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = self.parameters[\"W\"].shape\n",
    "\n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "\n",
    "    # Step 1: Initialize Gradients\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Step 2: Padding\n",
    "    A_prev_pad = zero_pad(A_prev, self.pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, self.pad)\n",
    "\n",
    "    # Step 3: Loop Through Training Examples\n",
    "    for i in range(m):                         # loop over the batch of training examples\n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "\n",
    "                    # Step 3-1: Extracting slices\n",
    "                    vert_start = h * self.stride\n",
    "                    vert_end = vert_start + self.filter_size\n",
    "                    horiz_start = w * self.stride\n",
    "                    horiz_end = horiz_start + self.filter_size\n",
    "                    a_slice = A_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Step 3-2: Update the Gradients\n",
    "                    dA_prev_pad[i, vert_start:vert_end, horiz_start:horiz_end, :] += self.parameters[\"W\"][:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += (1 / m)* a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += (1/m) * dZ[i, h, w, c]\n",
    "\n",
    "        # Step 4: Remove Padding\n",
    "        dA_prev[i, :, :, :] = dA_prev_pad[i, self.pad:-self.pad, self.pad:-self.pad, :]\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "    self.dW = dW\n",
    "    self.db = db\n",
    "\n",
    "    return dA_prev\n",
    "\n",
    "Conv.backward = backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "\n",
    "    Arguments:\n",
    "    learning rate -- step size\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "    self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "Conv.update = update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool():\n",
    "    def __init__(self, pool_size=2, stride=2):\n",
    "\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def create_mask_from_window(self, x):\n",
    "        \"\"\"\n",
    "        Creates a mask from an input x to identify the max entry of x.\n",
    "\n",
    "        Arguments:\n",
    "        x -- Array of shape (filter_size, filter_size)\n",
    "\n",
    "        Returns:\n",
    "        mask -- Array of the same shape as filter, contains a True at the position corresponding to the max entry of x.\n",
    "        \"\"\"\n",
    "\n",
    "        mask = x == np.max(x)\n",
    "\n",
    "        return mask\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "        \"\"\"\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        # retrieve dimensions from the input shape\n",
    "        (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "\n",
    "\n",
    "        # Step 1: Output Dimension Calculation\n",
    "        n_H = int((n_H_prev - self.pool_size)/self.stride) + 1\n",
    "        n_W = int((n_W_prev - self.pool_size)/self.stride) + 1\n",
    "        n_C = n_C_prev\n",
    "\n",
    "        # initialize output matrix A with zeros\n",
    "        A = np.zeros((m, n_H, n_W, n_C))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Applying Maxpooling\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Store the input in \"cache\" for backward pass\n",
    "        self.cache = A_prev\n",
    "\n",
    "        # Making sure your output shape is correct\n",
    "        assert(A.shape == (m, n_H, n_W, n_C))\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the max pooling layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "        \"\"\"\n",
    "\n",
    "        # Retrieve information from cache\n",
    "        A_prev = self.cache\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "\n",
    "        # Retrieve dimensions from A_prev's shape and dA's shape\n",
    "        m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "        m, n_H, n_W, n_C = dA.shape\n",
    "\n",
    "        # Step 1: Initialize Gradients\n",
    "        dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "\n",
    "        # Step 2: Loop Through Training Examples\n",
    "        for i in range(m):                           # loop over the batch of training examples\n",
    "            for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "                for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                    for c in range (n_C):            # loop over the channels of the output volume\n",
    "\n",
    "                        # Step 2-1: Extracting slices\n",
    "                        vert_start = h * self.stride\n",
    "                        vert_end = vert_start + self.pool_size\n",
    "                        horiz_start = w * self.stride\n",
    "                        horiz_end = horiz_start + self.pool_size\n",
    "                        a_prev_slice = A_prev[i][vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "\n",
    "                        # Step 2-2: Pass through the Gradients\n",
    "                        mask = self.create_mask_from_window(a_prev_slice)\n",
    "                        dA_prev[i][vert_start:vert_end, horiz_start:horiz_end, c] = mask * dA[i, h, w, c]\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Make sure your output shape is correct\n",
    "\n",
    "        assert(dA_prev.shape == A_prev.shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A_prev):\n",
    "        \"\"\"\n",
    "        Implements the forward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "\n",
    "        Returns:\n",
    "        A -- output of the flatten layer, a 2-dimensional array of shape (m, (n_H_prev * n_W_prev * n_C_prev))\n",
    "        \"\"\"\n",
    "\n",
    "        # Save information in \"cache\" for the backward pass\n",
    "        self.cache = A_prev.shape\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        A = A_prev.reshape(A_prev.shape[0], -1)\n",
    "        ### END CODE HERE ###\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        Implements the backward pass of the flatten layer\n",
    "\n",
    "        Arguments:\n",
    "        dA -- Input data, a 2-dimensional array\n",
    "\n",
    "        Returns:\n",
    "        dA_prev -- An array with its original shape (the output shape of its' previous layer).\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ###\n",
    "        dA_prev = dA.reshape(self.cache)\n",
    "        ### END CODE HERE ###\n",
    "        return dA_prev\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        self.layers=[]\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        for l in range(len(self.layers)):\n",
    "            A = self.layers[l].forward(A)\n",
    "        ### END CODE HERE ###\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        L = len(self.layers)\n",
    "\n",
    "        ### START CODE HERE ###\n",
    "        e = 10**(-5)\n",
    "        dAL = (-1)*(Y/(AL + e) - (1 - Y)/(1 - AL + e))\n",
    "        # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "        dZ = self.layers[L-1].backward(dA=dAL)\n",
    "        dA_prev = self.layers[L-2].backward(dZ)\n",
    "        # Loop from l=L-2 to l=0\n",
    "        for l in reversed(range(L-2)):\n",
    "            dA_prev = self.layers[l].backward(dA_prev)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "\n",
    "        # Only convolution layer and dense layer have to update parameters\n",
    "        ### START CODE HERE ###\n",
    "        for l in range(len(self.layers)):\n",
    "            if self.layers[l].__class__.__name__ == \"Conv\" or self.layers[l].__class__.__name__ == \"Dense\":\n",
    "                self.layers[l].update(learning_rate)\n",
    "        ### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.load to load the data from npz file\n",
    "### START CODE HERE ###\n",
    "data = np.load(\"data.npz\")\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "### END CODE HERE ###\n",
    "\n",
    "# plot first few images\n",
    "for i in range(9):\n",
    "    # define subplot\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    # plot raw pixel data\n",
    "    plt.imshow(X_train[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "    plt.title(y_train[i])\n",
    "# show the figure\n",
    "plt.show()\n",
    "\n",
    "# check the shape of training data and testing data\n",
    "print('Train: X=%s, y=%s' % (X_train.shape, y_train.shape))\n",
    "print('Test: X=%s' % (X_test.shape, ))\n",
    "\n",
    "#You can split training and validation set here using train_test_split (Optional)\n",
    "### START CODE HERE ###\n",
    "split_ratio = 0.9\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "x1_train = X_train[:int(X_train.shape[0] * split_ratio)]\n",
    "y1_train = y_train[:int(y_train.shape[0] * split_ratio)]\n",
    "x_val = X_train[int(X_train.shape[0] * split_ratio):]\n",
    "y_val = y_train[int(y_train.shape[0] * split_ratio):]\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \"\"\"\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "\n",
    "    Arguments:\n",
    "    X -- input data, of shape !!!!!!!!!!!(number of examples ,input size)!!!!!!!!!!!\n",
    "    Y -- true \"label\" vector, of shape (number of classes, number of examples)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "\n",
    "    Returns:\n",
    "    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[0]  # number of training examples\n",
    "    mini_batches = []\n",
    "\n",
    "    # GRADED CODE: Binary classification\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size :,]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "learning_rate = 3e-5\n",
    "num_iterations = 20\n",
    "batch_size = 64\n",
    "costs = []   # keep track of cost\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = Model()\n",
    "\n",
    "# Build the model by adding layers\n",
    "model=Model()\n",
    "model.add(Conv(filter_size=3, input_channel=3, output_channel=8, pad=1, stride=2))\n",
    "model.add(Activation(\"relu\", None))\n",
    "model.add(MaxPool(pool_size=2, stride=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, 1))\n",
    "model.add(Activation(\"sigmoid\", None))\n",
    "\n",
    "# Loop (gradient descent)\n",
    "for i in range(0, num_iterations):\n",
    "    print(\"epoch: \",i)\n",
    "    mini_batches = random_mini_batches(X_train, y_train, batch_size)\n",
    "\n",
    "    for batch in mini_batches:\n",
    "        x_batch, y_batch = batch\n",
    "\n",
    "        # forward\n",
    "        AL = model.forward(x_batch)\n",
    "\n",
    "        # compute cost\n",
    "        cost = compute_BCE_loss(AL, y_batch)\n",
    "\n",
    "        # backward\n",
    "        dA_prev = model.backward(AL, y_batch)\n",
    "\n",
    "        # update\n",
    "        model.update(learning_rate)\n",
    "\n",
    "    # AL = model.forward(A)\n",
    "    # dA_prev = model.backward(AL=AL, Y=Y)\n",
    "    print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    costs.append(cost)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
