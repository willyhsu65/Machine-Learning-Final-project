{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4720 + 2606 = 7326\n",
    "# import model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "from PIL import Image\n",
    "#from sklearn.metrics import f1_score\n",
    "#from matplotlib.animation import FuncAnimation\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename all file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_and_number(folder_path, new_name):\n",
    "    for count, filename in enumerate(os.listdir(folder_path)):\n",
    "        old_file = os.path.join(folder_path, filename)\n",
    "        # Ensure it's a file\n",
    "        if os.path.isfile(old_file):\n",
    "            # Get file extension\n",
    "            extension = os.path.splitext(filename)[1]\n",
    "            if extension != \".png\":\n",
    "                extension = \".png\"\n",
    "            # Create new file name\n",
    "            new_file = os.path.join(folder_path, f\"{new_name}_{count + 1}{extension}\")\n",
    "            # Rename the file\n",
    "            os.rename(old_file, new_file)\n",
    "    print(\"Files renamed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files renamed successfully!\n",
      "Files renamed successfully!\n"
     ]
    }
   ],
   "source": [
    "rename_and_number(\"H:\\我的雲端硬碟\\jay chou\\\\total_Jay\\\\\", \"RealJay\")\n",
    "rename_and_number(\"H:\\我的雲端硬碟\\jay chou\\\\total_notJay\", \"FakeJay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(image_arr, th=0.4):\n",
    "        image_arr[image_arr > th] = 1.0\n",
    "        image_arr[image_arr <= th] = 0.0\n",
    "        return image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     y_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m20000\u001b[39m:, \u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues      \u001b[38;5;66;03m# Get validation labels\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_train, X_val, y_train, y_val\n\u001b[1;32m----> 9\u001b[0m X_train, X_val, y_train, y_val \u001b[38;5;241m=\u001b[39m SplitData(\u001b[43mtrain\u001b[49m)\n\u001b[0;32m     10\u001b[0m X_test \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39miloc[:, :\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "def SplitData(data):\n",
    "\n",
    "    X_train = data.iloc[:20000, :3].values  # Get training features\n",
    "    X_val = data.iloc[20000:, :3].values    # Get validation features\n",
    "    y_train = data.iloc[:20000, 3].values   # Get training labels\n",
    "    y_val = data.iloc[20000:, 3].values      # Get validation labels\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "X_train, X_val, y_train, y_val = SplitData(train)\n",
    "X_test = test.iloc[:, :3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def image_resize(path):\n",
    "    # path : the image path you want to resize\n",
    "    # img_resized: img have resize to 360*360\n",
    "    #open picture\n",
    "    img = Image.open(path)\n",
    "    # resize to 360*360\n",
    "    img_resized = img.resize((360, 360), Image.LANCZOS)\n",
    "    return img_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_folder(folder_path):\n",
    "    # read images from 'folder_path'\n",
    "    # flat the image and push into 'images' array\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith('.jpg'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # 打開圖片\n",
    "                with Image.open(file_path) as img:\n",
    "                    # 將圖片轉為灰階\n",
    "                    img = img.convert('L')\n",
    "                    # 檢查圖片尺寸是否為 360x360\n",
    "                    if img.size != (360, 360):\n",
    "                        image_resize(file_path)\n",
    "                    if img.size == (360, 360):\n",
    "                        # 將圖片轉換為 np.array 並進行標準化 (值範圍從 0 到 1)\n",
    "                        img_array = np.array(img) / 255.0\n",
    "                        # 將圖片展平 (flatten)\n",
    "                        img_array = img_array.flatten()\n",
    "                        images.append(img_array)\n",
    "                        print(images)\n",
    "                        print(f'image size: {len(images[0])}')\n",
    "                    else:\n",
    "                        print(f\"圖片尺寸為 {img.size}, 不是 360x360: {filename}\")\n",
    "            except UnidentifiedImageError:\n",
    "                print(f\"無法識別圖片文件: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"無法讀取圖片: {filename}, 錯誤: {e}\")\n",
    "    \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "圖片尺寸為 (423, 432), 不是 360x360: RealJay_1.jpg\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"G:\\我的雲端硬碟\\ML test\\\\\"\n",
    "preprocess_photo = read_images_from_folder(folder_path)\n",
    "for i in range(len(preprocess_photo)):\n",
    "    preprocess_photo[i] = _preprocess(preprocess_photo[i])\n",
    "#preprocess_photo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "# CNN\n",
    "# SVM\n",
    "def Model():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense():\n",
    "    def __init__(self, n_x, n_y, seed=1):\n",
    "        self.n_x = n_x\n",
    "        self.n_y = n_y\n",
    "        self.seed = seed\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "\n",
    "        sd = np.sqrt(6.0 / (self.n_x + self.n_y))\n",
    "        np.random.seed(self.seed)\n",
    "        W = np.random.uniform(-sd, sd, (self.n_y, self.n_x)).T      # the transpose here is just for the code to be compatible with the old codes\n",
    "        b = np.zeros((1, self.n_y))\n",
    "\n",
    "        assert(W.shape == (self.n_x, self.n_y))\n",
    "        assert(b.shape == (1, self.n_y))\n",
    "\n",
    "        self.parameters = {\"W\": W, \"b\": b}\n",
    "\n",
    "    def forward(self, A):\n",
    "        # GRADED FUNCTION: linear_forward\n",
    "        ### START CODE HERE ###\n",
    "        W = self.parameters[\"W\"]\n",
    "        b = self.parameters[\"b\"]\n",
    "        Z = np.dot(A, W) + b # matrix multiplication and addition\n",
    "        self.cache = (A, W, b)\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert(Z.shape == (A.shape[0], self.parameters[\"W\"].shape[1]))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, dZ):\n",
    "\n",
    "        A_prev, W, b = self.cache\n",
    "        m = A_prev.shape[0]\n",
    "\n",
    "        # GRADED FUNCTION: linear_backward\n",
    "        ### START CODE HERE ###\n",
    "        self.dW = (1 / m) * np.dot(A_prev.T, dZ) #gradient of loss with respect to weights\n",
    "        self.db = (1 / m) * np.sum(dZ, axis=0, keepdims=True) #gradient of loss with respect to bias\n",
    "        dA_prev = np.dot(dZ, W.T)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        assert (dA_prev.shape == A_prev.shape)\n",
    "        assert (self.dW.shape == self.parameters[\"W\"].shape)\n",
    "        assert (self.db.shape == self.parameters[\"b\"].shape)\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "      # GRADED FUNCTION: linear_update_parameters\n",
    "      ### START CODE HERE ###\n",
    "      self.parameters[\"W\"] = self.parameters[\"W\"] - learning_rate * self.dW\n",
    "      self.parameters[\"b\"] = self.parameters[\"b\"] - learning_rate * self.db\n",
    "      ### END CODE HERE ###\n",
    "class Activation():\n",
    "    def __init__(self, activation_function, loss_function):\n",
    "        self.activation_function = activation_function\n",
    "        self.loss_function = loss_function\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, Z):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            # GRADED FUNCTION: sigmoid_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.where(Z >= 0, 1 / (1 + np.exp(-Z)), np.exp(Z) / (1 + np.exp(Z)))\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"relu\":\n",
    "            # GRADED FUNCTION: relu_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.maximum(0, Z)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert(A.shape == Z.shape)\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            # GRADED FUNCTION: softmax_forward\n",
    "            ### START CODE HERE ###\n",
    "            A = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            self.cache = Z\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            return A\n",
    "        elif self.activation_function == \"linear\":\n",
    "            self.cache = Z.copy()\n",
    "            return Z\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def backward(self, dA=None, Y=None):\n",
    "        if self.activation_function == \"sigmoid\":\n",
    "            # GRADED FUNCTION: sigmoid_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            A = 1 / (1 + np.exp(-Z))\n",
    "            dZ = dA * A * (1 - A)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"relu\":\n",
    "            # GRADED FUNCTION: relu_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            dZ = dA * (Z > 0)\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == Z.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"softmax\":\n",
    "            # GRADED FUNCTION: softmax_backward\n",
    "            ### START CODE HERE ###\n",
    "            Z = self.cache\n",
    "            s = np.exp(Z - np.max(Z, axis=1, keepdims=True)) / np.sum(np.exp(Z - np.max(Z, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "            dZ = s - Y\n",
    "            ### END CODE HERE ###\n",
    "\n",
    "            assert (dZ.shape == self.cache.shape)\n",
    "\n",
    "            return dZ\n",
    "\n",
    "        elif self.activation_function == \"linear\":\n",
    "            return dA\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_function}\")\n",
    "\n",
    "class Model():\n",
    "    def __init__(self, units, activation_functions, loss_function):\n",
    "        self.units = units\n",
    "        self.activation_functions = activation_functions\n",
    "        self.loss_function = loss_function\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        self.linear = []        # Store all Dense layers (weights & biases)\n",
    "        self.activation = []    # Store all activation function layers\n",
    "\n",
    "        for i in range(len(self.units)-1):\n",
    "            dense = Dense(self.units[i], self.units[i+1], i)\n",
    "            self.linear.append(dense)\n",
    "\n",
    "        for i in range(len(self.activation_functions)):\n",
    "            self.activation.append(Activation(self.activation_functions[i], self.loss_function))\n",
    "\n",
    "    def forward(self, X):\n",
    "        A = X\n",
    "\n",
    "        # GRADED FUNCTION: model_forward\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(len(self.linear)):\n",
    "            A = self.linear[i].forward(A)\n",
    "            A = self.activation[i].forward(A)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, AL=None, Y=None):\n",
    "        L = len(self.linear)\n",
    "        C = Y.shape[1]\n",
    "\n",
    "        # assertions\n",
    "        warning = 'Warning: only the following 3 combinations are allowed! \\n \\\n",
    "                    1. binary classification: sigmoid + cross_entropy \\n \\\n",
    "                    2. multi-class classification: softmax + cross_entropy \\n \\\n",
    "                    3. regression: linear + mse'\n",
    "        assert self.loss_function in [\"cross_entropy\", \"mse\"], \"you're using undefined loss function!\"\n",
    "        if self.loss_function == \"cross_entropy\":\n",
    "            if Y.shape[1] == 1:  # binary classification\n",
    "                assert self.activation_functions[-1] == 'sigmoid', warning\n",
    "            else:  # multi-class classification\n",
    "                assert self.activation_functions[-1] == 'softmax', warning\n",
    "                assert self.units[-1] == Y.shape[1], f\"you should set last dim to {Y.shape[1]}(the number of classes) in multi-class classification!\"\n",
    "        elif self.loss_function == \"mse\":\n",
    "            assert self.activation_functions[-1] == 'linear', warning\n",
    "            assert self.units[-1] == Y.shape[1], \"output dimension mismatch for regression!\"\n",
    "\n",
    "        # GRADED FUNCTION: model_backward\n",
    "        ### START CODE HERE ###\n",
    "        if self.activation_functions[-1] == \"linear\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = AL - Y\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ=dAL)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"sigmoid\":\n",
    "            # Initializing the backpropagation\n",
    "            dAL = -(Y / (AL + 1e-5) - (1 - Y) / (1 - AL + 1e-5))\n",
    "\n",
    "            # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL\". Outputs: \"dA_prev\"\n",
    "            dZ = self.activation[-1].backward(dA=dAL)\n",
    "            dA_prev = self.linear[-1].backward(dZ=dZ)\n",
    "\n",
    "        elif self.activation_functions[-1] == \"softmax\":\n",
    "            # Initializing the backpropagation\n",
    "            dZ = self.activation[-1].backward(Y=Y)\n",
    "\n",
    "            # Lth layer (LINEAR) gradients. Inputs: \"dZ\". Outputs: \"dA_prev\"\n",
    "            dA_prev = self.linear[-1].backward(dZ=dZ)\n",
    "\n",
    "        # Loop from l=L-2 to l=0\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"dA_prev\". Outputs: \"dA_prev\"\n",
    "        for i in (range(L-2, -1, -1)):\n",
    "            dZ = self.activation[i].backward(dA=dA_prev)\n",
    "            dA_prev = self.linear[i].backward(dZ=dZ)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        return dA_prev\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        L = len(self.linear)\n",
    "\n",
    "        # GRADED FUNCTION: model_update_parameters\n",
    "        ### START CODE HERE ###\n",
    "        for i in range(L):\n",
    "            self.linear[i].update(learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "# GRADED FUNCTION: compute_BCE_loss\n",
    "\n",
    "def compute_BCE_loss(AL, Y):\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(np.multiply(Y, np.log(AL + 1e-5)) + np.multiply(1 - Y, np.log(1 - AL + 1e-5)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss\n",
    "# GRADED FUNCTION: compute_CCE_loss\n",
    "\n",
    "def compute_CCE_loss(AL, Y):\n",
    "    n = Y.shape[0]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = -(1/n) * np.sum(np.multiply(Y, np.log(AL + 1e-5)))\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    loss = np.squeeze(loss)      # To make sure your loss's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(loss.shape == ())\n",
    "\n",
    "    return loss\n",
    "    \n",
    "# compute_MSE_loss (MSE)\n",
    "def compute_MSE_loss(AL, Y):\n",
    "    m = Y.shape[0]\n",
    "    loss = (1/m) * np.sum(np.square(AL - Y))\n",
    "    return loss\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64):\n",
    "    \n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    # Cases with a complete mini batch size only i.e each of 64 examples.\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : (k+1) * mini_batch_size, :]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : (k+1) * mini_batch_size, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m, :]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mini_batches\n",
    "\n",
    "def train_model(model, X_train, Y_train, learning_rate, num_iterations, batch_size=None, print_loss=True, print_freq=1000, decrease_freq=100, decrease_proportion=0.99):\n",
    "    history = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        ### START CODE HERE ###\n",
    "        # Define mini batches\n",
    "        if batch_size:\n",
    "            mini_batches = random_mini_batches(X_train, Y_train, batch_size)\n",
    "        else:\n",
    "            # if batch_size is None, batch is not used, mini_batch = whole dataset\n",
    "            mini_batches = [(X_train, Y_train)]\n",
    "\n",
    "        epoch_loss = 0\n",
    "        for batch in mini_batches:\n",
    "            X_batch, Y_batch = batch\n",
    "\n",
    "            # Forward pass\n",
    "            AL = model.forward(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            if model.loss_function == 'cross_entropy':\n",
    "                if model.activation_functions[-1] == \"sigmoid\": # Binary classification\n",
    "                    loss = compute_BCE_loss(AL, Y_batch)\n",
    "                elif model.activation_functions[-1] == \"softmax\": # Multi-class classification\n",
    "                    loss = compute_CCE_loss(AL, Y_batch)\n",
    "            elif model.loss_function == 'mse': # Regression\n",
    "                loss = compute_MSE_loss(AL, Y_batch)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            # Backward pass\n",
    "            model.backward(AL, Y_batch)\n",
    "\n",
    "            # Update parameters\n",
    "            model.update(learning_rate)\n",
    "\n",
    "        epoch_loss /= len(mini_batches)\n",
    "        losses.append(epoch_loss)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Print loss\n",
    "        if print_loss and i % print_freq == 0:\n",
    "            print(f\"Loss after iteration {i}: {epoch_loss}\")\n",
    "\n",
    "        # Store history\n",
    "        if i % 100 == 0:\n",
    "            history.append((X_train, model.forward(X_train)))\n",
    "\n",
    "        # Decrease learning rate\n",
    "        if i % decrease_freq == 0 and i > 0:\n",
    "            learning_rate *= decrease_proportion\n",
    "\n",
    "    return model, losses, history\n",
    "def predict(x, y_true, model):\n",
    "    n = x.shape[0]\n",
    "\n",
    "    # Forward propagation\n",
    "    y_pred = model.forward(x)\n",
    "\n",
    "    if y_pred.shape[-1] == 1:\n",
    "        y_pred = np.array([[1 - y[0], y[0]] for y in y_pred])\n",
    "        if y_true is not None:\n",
    "            y_true = np.array([[1,0] if y == 0 else [0,1] for y in y_true.reshape(-1)])\n",
    "\n",
    "    # make y_pred/y_true become one-hot prediction result\n",
    "    if y_true is not None:\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "    y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    if y_true is not None:\n",
    "        # compute accuracy\n",
    "        correct = 0\n",
    "        for yt, yp in zip(y_true, y_pred):\n",
    "            if yt == yp:\n",
    "                correct += 1\n",
    "        print(f\"Accuracy: {correct/n * 100:.2f}%\")\n",
    "\n",
    "        f1_scores = f1_score(y_true, y_pred, average=None)\n",
    "        print(f'f1 score for each class: {f1_scores}')\n",
    "        print(f'f1_macro score: {np.mean(np.array(f1_scores)):.2f}')\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "def save_prediction_data(predicted_y):\n",
    "    # Create DataFrame with ID, x, and y columns\n",
    "    df = pd.DataFrame({\n",
    "        'ID': range(len(predicted_y)),  # Add ID column starting from 0\n",
    "        'y': predicted_y\n",
    "    })\n",
    "\n",
    "    # Ensure ID is the first column\n",
    "    df = df[['ID', 'y']]\n",
    "\n",
    "    # Save to CSV file\n",
    "    df.to_csv('Lab4_basic_regression.csv', index=False)\n",
    "    print(\"Prediction data saved as 'Lab4_basic_regression.csv'\")\n",
    "\n",
    "def animate_training(history, X_train, Y_train):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_xlim(0, 11)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    line, = ax.plot([], [], 'b-', lw=1, label='Predicted')\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "    ax.plot(ground_truth_x, ground_truth_y, 'r-', lw=1, label='Ground Truth')\n",
    "\n",
    "    # show current epoch on the animation / 100 epoch\n",
    "    epoch_text = ax.text(0.05, 0.95, '', transform=ax.transAxes, fontsize=12, verticalalignment='top')\n",
    "\n",
    "    def init():\n",
    "        line.set_data([], [])\n",
    "        epoch_text.set_text('')\n",
    "        return line, epoch_text\n",
    "\n",
    "    def update(frame):\n",
    "        epoch = (frame + 1) * 100\n",
    "        _, predicted_y = history[frame]\n",
    "        predicted_x = X_train.flatten()\n",
    "        line.set_data(predicted_x, predicted_y.flatten())\n",
    "\n",
    "        epoch_text.set_text(f'Epoch: {epoch}')\n",
    "\n",
    "        return line, epoch_text\n",
    "\n",
    "    ani = FuncAnimation(fig, update, frames=len(history), init_func=init, blit=True, interval=50)\n",
    "\n",
    "    # save as gif\n",
    "    ani.save('Lab4_basic_regression.gif', writer='pillow')\n",
    "    plt.close(fig)\n",
    "    print(f\"Animation saved as 'Lab4_basic_regression.gif'\")\n",
    "\n",
    "\n",
    "def save_final_result(model, X_train, Y_train):\n",
    "    AL = model.forward(X_train)\n",
    "\n",
    "    predicted_x = X_train.flatten()\n",
    "    predicted_y = AL.flatten()\n",
    "\n",
    "    plt.plot(predicted_x, predicted_y, 'b-', label=\"Predicted\", lw=1)\n",
    "\n",
    "    ground_truth_x = X_train.flatten()\n",
    "    ground_truth_y = Y_train.flatten()\n",
    "\n",
    "    save_prediction_data(predicted_y)\n",
    "\n",
    "    plt.plot(ground_truth_x, ground_truth_y, 'r-', label='Ground Truth', lw=1)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.ylim(-5, 5)\n",
    "    plt.xlim(0, 11)\n",
    "    plt.savefig(\"Lab4_basic_regression.jpg\")\n",
    "    plt.show()\n",
    "    print(\"Prediction saved as 'Lab4_basic_regression.jpg'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
